{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n",
      "1.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import  tools\n",
    "import aug\n",
    "    \n",
    "data = np.load(\"../data/VITPOSE/1/data.npy\")\n",
    "# data = np.transpose(source, (0, 4, 2, 3, 1))\n",
    "\n",
    "data = data[0]\n",
    "\n",
    "data = aug.random_scaling_2d(data)\n",
    "# data = aug.random_translation_2d(data)\n",
    "# data = aug.random_rotation_2d(data)\n",
    "\n",
    "\n",
    "# M, T, V, C = data.shape\n",
    "# Mask = np.zeros((T, V, C))\n",
    "# joint = np.array([0, 1, 3, 5, 7, 9, 11, 13, 15])\n",
    "# Mask[:, joint, :] = 1\n",
    "# x = data[0] * Mask + data[1] * (1-Mask)\n",
    "\n",
    "data = np.transpose(data, (3, 1, 2, 0))\n",
    "tools.coco_one_person_video(data[1], \"15.mp4\", fps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31200, 3, 16, 17, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (923, 924) to (928, 928) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      " 12%|█▎        | 2/16 [00:00<00:02,  4.98it/s][rawvideo @ 0x6f2e740] Stream #0: not enough frames to estimate rate; consider increasing probesize\n",
      "[swscaler @ 0x6f46900] Warning: data is not aligned! This can lead to a speed loss\n",
      "100%|██████████| 16/16 [00:03<00:00,  4.99it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import  tools\n",
    "import aug\n",
    "\n",
    "data = np.load(\"../data/MB_3DP/1/data.npy\").astype(np.float32)\n",
    "print(data.shape)\n",
    "data = aug.random_noise(data[0])\n",
    "# data = aug.random_scaling(data)\n",
    "# data = aug.random_translation(data)\n",
    "# data = aug.joint_courruption(data)\n",
    "# data = aug.shear(data)\n",
    "\n",
    "data = np.transpose(data, (3, 1, 2, 0))\n",
    "tools.motion2video_3d3(data[0], save_path=\"ジョイント汚染移動回転スケーリング_3.mp4\", fps=2, view=[15, 105])\n",
    "# tools.coco_one_person_video(data[0], \"m3.mp4\", fps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickleファイル作成後の可視化\n",
    "\n",
    "from mmcv import load, dump\n",
    "import numpy as np\n",
    "import tools\n",
    "\n",
    "a = load(\"../data_gen/VITPOSE_SCORE_4pair.pkl\")\n",
    "data = a[\"annotations\"][99]\n",
    "\n",
    "print(data[\"frame_dir\"])\n",
    "tools.coco_one_person_video(data['keypoint'][0, :, :, :2], \"a.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import  tools\n",
    "import aug\n",
    "\n",
    "source = np.load(\"../data/normarized_data/MB_3DP.npy\")\n",
    "print(source.shape)\n",
    "# source = np.load(\"../data/SMPL2H36M2/1/data.npy\")\n",
    "data = np.transpose(source, (0, 4, 2, 3, 1))\n",
    "# tools.motion2video_3d2(source[0, 0, :300:10], save_path=\"before.mp4\", fps=2, view=[0, 90])\n",
    "\n",
    "for i in range(0, 624, 10):\n",
    "    tools.motion2video_3d2(data[i,0, :300:50], save_path=f\"{i}.mp4\", fps=2, view=[115, 180])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データオーギュメント後のデータ可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2742, -0.1540, -0.0983])\n",
      "(2, 16, 17, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (923, 924) to (928, 928) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      " 12%|█▎        | 2/16 [00:00<00:03,  4.45it/s][rawvideo @ 0x6baa740] Stream #0: not enough frames to estimate rate; consider increasing probesize\n",
      "[swscaler @ 0x6bc26c0] Warning: data is not aligned! This can lead to a speed loss\n",
      "100%|██████████| 16/16 [00:03<00:00,  4.87it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tools\n",
    "import aug\n",
    "\n",
    "source = np.load(\"../data/MB_3DP/1/data.npy\").astype(np.float32)\n",
    "# print(source.shape)\n",
    "n, c, t, v, m = source.shape\n",
    "data = source[0].copy()\n",
    "# print(data.shape)\n",
    "# data = np.transpose(data, (3, 1, 2, 0))\n",
    "\n",
    "# data = aug.Flip(data)\n",
    "data = aug.joint_courruption(data)\n",
    "data = aug.random_rot(data)\n",
    "\n",
    "# data = aug.gaussian_noise(data)\n",
    "# data = aug.random_scaling(data)\n",
    "# data = aug.random_translation(data)\n",
    "# data = aug.gaussian_noise(data)\n",
    "\n",
    "data = np.transpose(data, (3, 1, 2, 0))\n",
    "print(data.shape)\n",
    "# data = np.transpose(data, (0, 2, 3, 1))\n",
    "\n",
    "# m, t, v, c\n",
    "# v, c, t\n",
    "\n",
    "# tools.motion2video_3d2(source[0, 0], save_path=\"before.mp4\", fps=2, view=[15, 105])\n",
    "tools.motion2video_3d3(data[0], save_path=\"kumi4-3.mp4\", fps=2, view=[15, 105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "# video_name : 動画の名前 ex 1-1-1-2\n",
    "# frame_lst : 取り出したいフレームのリスト\n",
    "def create_video(video_name, frame_lst):\n",
    "        \n",
    "    # 複数のpng画像からgif画像を生成\n",
    "    images = []\n",
    "    for i in frame_lst:\n",
    "        filename = '{}-{:03}.png'.format(video_name, i)\n",
    "        img = Image.open(filename)\n",
    "        images.append(img)\n",
    "    images[0].save('out.gif', save_all=True, append_images=images[1:], loop=0, duration=150)\n",
    "    \n",
    "    # 表示\n",
    "    # with open('out.gif','rb') as f:\n",
    "    #     display.Image(data=f.read(), format='png')\n",
    "        \n",
    "        \n",
    "def extract_frame(video_path, frame_number, img_result_path):\n",
    "    # 動画ファイルを開く\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # フレームを指定位置まで進める\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    \n",
    "    # 指定のフレームを読み込む\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        # フレームを保存する\n",
    "        cv2.imwrite(img_result_path, frame)\n",
    "        print(\"Frame extracted successfully.\")\n",
    "    else:\n",
    "        print(\"Failed to extract frame.\")\n",
    "    \n",
    "    # メモリを解放して、動画を閉じる\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "# save_frame('data/temp/sample_video.mp4', 100, 'data/temp/result_single/sample_100.jpg')\n",
    "\n",
    "fb = 1\n",
    "pos = 1\n",
    "pr = 1\n",
    "lb = 1\n",
    "\n",
    "interval_num = 18\n",
    "out_frame_num = 1\n",
    "lst = list(range(5, interval_num*out_frame_num, interval_num))  # {interval_num}間隔で{out_frame_num}だけ取り出す\n",
    "\n",
    "print(lst)\n",
    "\n",
    "\n",
    "for fb in range(1, 3):\n",
    "    for pos in range(1, 3):\n",
    "        for pr in range(1, 53):\n",
    "            for lb in range(1, 4):\n",
    "\n",
    "                # ここからが画像を保存してgifを作成する処理\n",
    "                video_name='result/{}-{}-{}-{}'.format(fb, pos, pr, lb)\n",
    "                video_path = f\"../../../video/{fb}/{pos}/{pr}/{lb}/Resize256/{lb}.mp4\"\n",
    "                \n",
    "                for l in lst:\n",
    "                    img_result_path='{}-{:03d}.png'.format(video_name, l)\n",
    "                    print(img_result_path)\n",
    "                    extract_frame(video_path, l, img_result_path)\n",
    "                    \n",
    "                create_video(video_name, lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "def plot_graph(input, att_node):\n",
    "    for i in range(16):\n",
    "        one_person_3d_img(input=input[0, 0, i], att_node=att_node[0, 0, i], filename=i)\n",
    "        \n",
    "    # 複数のpng画像からgif画像を生成\n",
    "    images = []\n",
    "    for i in range(16):\n",
    "        filename = '{:03}.png'.format(i)\n",
    "        img = Image.open(filename)\n",
    "        images.append(img)\n",
    "    images[0].save('out.gif', save_all=True, append_images=images[1:], loop=0, duration=80)\n",
    "    \n",
    "    # 表示\n",
    "    with open('out.gif','rb') as f:\n",
    "        display.Image(data=f.read(), format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "投影の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection(data, theta=0.3):\n",
    "    \"\"\"\n",
    "    data_numpy: C,T,V,M\n",
    "    \"\"\"\n",
    "    \n",
    "    # プロジェクション行列を定義\n",
    "    P_x = np.array([[0, 0, 0],\n",
    "                    [0, 1, 0],\n",
    "                    [0, 0, 1]])\n",
    "\n",
    "    P_y = np.array([[1, 0, 0],\n",
    "                    [0, 0, 0],\n",
    "                    [0, 0, 1]])\n",
    "\n",
    "    P_z = np.array([[1, 0, 0],\n",
    "                    [0, 1, 0],\n",
    "                    [0, 0, 0]])\n",
    "    \n",
    "    data = np.transpose(data, (3, 1, 2, 0))\n",
    "    \n",
    "    projection_matrices = [P_x, P_y, P_z]\n",
    "    selected_projection_matrix = projection_matrices[random.choice([0, 1, 2])]\n",
    "    # 各関節の座標にプロジェクションを適用\n",
    "    data = np.dot(data, selected_projection_matrix.T)\n",
    "    \n",
    "    data = np.transpose(data, (3, 1, 2, 0))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def random_translation(data, theta=0.2):\n",
    "    import numpy as np\n",
    "\n",
    "    # 平行移動の範囲を設定\n",
    "    translation_range = [-theta, theta]\n",
    "\n",
    "    data = np.transpose(data, (3, 1, 2, 0))\n",
    "    \n",
    "    # x軸およびz軸方向のランダムな平行移動値 Δ を生成\n",
    "    delta_x = np.random.uniform(translation_range[0], translation_range[1])\n",
    "    delta_z = np.random.uniform(translation_range[0], translation_range[1])\n",
    "\n",
    "    # 平行移動ベクトル Δ を作成\n",
    "    delta = np.array([delta_x, 0, delta_z])\n",
    "\n",
    "    # 各関節の座標に平行移動を適用\n",
    "    data = data + delta\n",
    "    \n",
    "    data = np.transpose(data, (3, 1, 2, 0))\n",
    "    return data\n",
    "\n",
    "\n",
    "def random_scaling(data, scale_range=(0.8, 2.0)):\n",
    "\n",
    "    data = np.transpose(data, (3, 1, 2, 0))\n",
    "    m, t, v, c = data.shape\n",
    "    \n",
    "    # スケーリングファクターをランダムに生成\n",
    "    scale_factors = np.random.uniform(scale_range[0], scale_range[1], size=(m, t, 1, 3))\n",
    "    \n",
    "    # スケーリングを適用\n",
    "    data = data * scale_factors\n",
    "    \n",
    "    data = np.transpose(data, (3, 1, 2, 0))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tools\n",
    "import aug\n",
    "\n",
    "\n",
    "# 例として、3Dスケルトンシーケンスデータ（各行が1つの関節の座標を表す）\n",
    "# 例: 1つのフレームで4つの関節がある場合\n",
    "source = np.load(\"../data/MB_3DP/1/data.npy\")\n",
    "\n",
    "# print(source.shape)\n",
    "\n",
    "a = aug.shear(source[0])\n",
    "a = np.transpose(a, (3, 1, 2, 0))\n",
    "\n",
    "print(a.shape)\n",
    "tools.motion2video_3d3(a[0], save_path=\"s3.mp4\", fps=2, view=[15, 105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imageio\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# サンプルの[T, V, C]データを作成（T=時間フレーム、V=関節数、C=座標数）\n",
    "T = 10  # 時間フレーム数\n",
    "V = 5   # 関節数\n",
    "C = 2   # 座標（2Dの場合）\n",
    "\n",
    "# ランダムな骨格データ（例としてランダムな座標値を生成）\n",
    "skeleton_data = np.random.rand(T, V, C) * 100  # 座標範囲は[0, 100]と仮定\n",
    "\n",
    "# matplotlibにおけるfigからopencvにおけるimgを作成\n",
    "def get_img_from_fig(fig, dpi=120):\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf, format=\"png\", dpi=dpi, bbox_inches=\"tight\", pad_inches=0)\n",
    "    buf.seek(0)\n",
    "    img_arr = np.frombuffer(buf.getvalue(), dtype=np.uint8)\n",
    "    buf.close()\n",
    "    img = cv2.imdecode(img_arr, 1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGBA)\n",
    "    return img\n",
    "\n",
    "\n",
    "videowriter = imageio.get_writer(\"a.mp4\", fps=2)\n",
    "\n",
    "# 時間フレームごとに可視化\n",
    "for t in range(T):\n",
    "    fig = plt.figure()\n",
    "    plt.scatter(skeleton_data[t, :, 0], skeleton_data[t, :, 1], c=range(V), cmap='viridis', s=100)\n",
    "    plt.title(f\"Frame {t + 1}\")\n",
    "    plt.colorbar(label='Joint Index')  # カラーバーを追加\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlabel('X-coordinate')\n",
    "    plt.ylabel('Y-coordinate')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    frame_vis = get_img_from_fig(fig)\n",
    "    videowriter.append_data(frame_vis)\n",
    "    plt.close()\n",
    "videowriter.close()\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyskl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
